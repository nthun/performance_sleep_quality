---
title: "Sleep quality and cognitve performance analysis"
author: "Tamas Nagy"
date: "January 21, 2019"
bibliography: "refs/refs.bib" 
output: 
  html_document:
   toc: true
   toc_float: true
editor_options: 
  chunk_output_type: console
---

# Aim
We want to investigate the effect of poor sleep quality on cognitive performance. 
Two sleep quality questionnaires and several cognitive tests were administered across 
three studies.  

First, we want to aggregate the sleep quality questionnaires into one 
sleep quality metric. Then we want to investigate the association while controlling for 
the potential effect of the separate studies. Subsequently, we want to obtain 
Bayes Factors, so we can draw conclusions about the probability of null or alternative 
hypothesis.


```{r setup, include=FALSE}
# install.packages(c("tidyverse", "janitor","psych","lme4"))

library(tidyverse)
library(lme4)

# Define variable labels. This is important for making the plots pretty
variables_needed <- 
  c(
    "RT Triplet learning" = "trip_learn_all_rt",
    "ACC Triplet learning" = "trip_learn_all_acc",                 
    "ACC Higher-order sequence learning" = "highrer_order_acc",
    "RT Higher-order sequence learning" = "highrer_order_rt",
    "ACC Statistical learning" = "stat_learn_acc",
    "RT Statistical learning" = "stat_learn_rt",
    "Average ACC" = "acc_avg",
    "ACC general skill learning" = "acc_gs",
    "RT average" = "rt_avg",
    "RT general skill learning" = "rt_gs_1min4",
    "Counting Span" = "cs_avg",
    "WCST – perseverative error" = "wcst_pers_error",
    "Sleep disturbance" = "sleep_disturb",
    "study"
  )

variable_groups <- 
  tribble(~outcome_var, ~group,
          "RT Triplet learning", "RT learning indices",
          "ACC Triplet learning", "ACC learning indices",                 
          "ACC Higher-order sequence learning", "ACC learning indices",
          "RT Higher-order sequence learning", "RT learning indices",
          "ACC Statistical learning", "ACC learning indices",
          "RT Statistical learning", "RT learning indices",
          "Average ACC", "General skill indices",
          "ACC general skill learning", "General skill indices",
          "RT average", "General skill indices",
          "RT general skill learning", "General skill indices",
          "Counting Span", "WM and EF indices",
          "WCST – perseverative error", "WM and EF indices"
  )

```

```{r data_read, message=FALSE, warning=FALSE}
df_raw <- read_csv2("data/PSQIcikkhez adatok.csv", 
                    locale = locale(encoding = "WINDOWS-1252"))

# Process data for ease of use
# Clean variable names: Remove spaces, accented characters, capitals, make smallcase
# Translate Hungarian variable names, recode sex

df <-
  df_raw %>% 
  janitor::clean_names() %>% 
  rename(sex = nem, age = eletkor, education_yrs = iskolazottsag) %>% 
  mutate(sex = recode(sex, `1` = "Male", `2` = "Female", `0` = "Not specified"))
```

# Principal component analysis of sleep quality

We conduct a principal component analysis (PCA) to aggregate the two sleep quality 
measures into one. 

```{r}
sleepd_pca <- 
  df %>% 
  select(ais_ossz, psqi_osszpont) %>% 
  psych::pca(nfactors = 1)

# Show PCA result
sleepd_pca

bartlett_result <- 
  df %>% 
  select(ais_ossz, psqi_osszpont) %>% 
  cor() %>% 
  psych::cortest.bartlett(n = 235)

# Add the PC to the original data 
df <- 
  df %>% 
  mutate(sleep_disturb = sleepd_pca$scores[,1])
```

The Bartlett's test of shericity indicated that the correlation between the scales were
adequately large for a PCA, $\chi^2(235)$ = `r round(bartlett_result$chisq, 2)`, p < .0001.

One principal factor with an eigenvalue of `r round(sleepd_pca$values[1], 2)` was extracted to represent 
sleep disturbance. The component explained `r scales::percent(sleepd_pca$fit.off)` of the 
variance, and it was named sleep disturbance. 

# Exploraton of important variables
```{r}
skimr::skim_with(numeric = list(hist = NULL)) # Remove spark histograms
df %>% 
  select(one_of(variables_needed)) %>% 
  skimr::skim()
```

# Checking hypotheses
First, we create several linear mixed-effects models (LMEM) with the same predictor (sleep disturbance), and the cognitive performance metrics as outcomes. The LMEM contains a 
random intercept term by study to account for the potential differences between studies.

```{r warning=FALSE}
all_models <- 
  df %>% 
  select(one_of(variables_needed)) %>% 
  gather(outcome_var, value, -sleep_disturb, -study) %>% 
  split(.$outcome_var) %>% 
  map(., ~lmer(scale(value) ~ scale(sleep_disturb) + (1|study), REML = FALSE,
               data = .x))

# From the models, we can get the  standardized betas for the slopes and p the values
# We recalculate the models using restricted likelihood for this, so the estimated parameters will be more accurate

models <- 
  all_models %>% 
  map(~update(.x, REML = TRUE)) %>% 
  map(~broom::tidy(.)) %>% 
  bind_rows(.id = "outcome_var") %>% 
  filter(!str_detect(term, "(Intercept)|.Residual")) %>% 
  # Add number of observations by hand and calculate the two-sided p statistic 
  # based on t value and residual df
  mutate(n = c(rep(231, 11), 225),
         p = 2*pt(q = -abs(statistic), df = n - 1))

knitr::kable(models, digits = 4)
```

The LMEMs show no evidences that the cognitive metrics were affected by poor sleep quality.

# Calculate Bayes Factors ($BF_{01}$)

In order to make prediction about the likelihood of the null hypothesis - as compared to
the alternative hypothesis - we calculated the Bayes Factor [@Wagenmakers2007].
To calculate the BIC derived Bayes Factor, we calculated the Bayesian Information Criterion (BIC) for null models that only included a fixed intercept, and a random intercept term by study. Maximum Likelihood estimation was used so the models could be compared. The $BF_{01}$ were calculated based on @Wagenmakers2007. 

```{r}
# BF was calculated based on this tutoerial: https://rpubs.com/lindeloev/bayes_factors
# Calculate BIC for NULL models
null_BIC <-
  df %>%
  select(one_of(variables_needed)) %>% 
  gather(outcome_var, value, -sleep_disturb, -study) %>% 
  split(.$outcome_var) %>%
  map_df(., ~lmer(scale(value) ~ 1 + (1|study), REML = FALSE, 
                  data = .x) %>% 
           broom::glance(.) %>% 
           select(nullBIC = BIC),
         .id = "outcome_var") 

# Calculate BF01 for all models
all_models %>% 
  map_df(~broom::glance(.),
         .id = "outcome_var") %>%
  left_join(null_BIC, by = "outcome_var") %>%
  mutate(BF_BIC = exp((BIC - nullBIC)/2)) %>% 
  select(outcome_var, BF_BIC) %>% 
  knitr::kable(digits = 4)

```

As all BFs were over 3, we can conclude that the cognitive performance of the study
population seems to be unaffected by poor sleep.

# Visualize the association with sleep disturbance for each outcome measure

(I wannted to make this prettier, but there are a few bugs that make it difficult, 
will update if I find the solution)
```{r fig.width=9}
plot_df <-
  df %>%
  # Use pretty variable names
  select(variables_needed) %>% 
  # Put data into long format so the outcome vriable can be faceted
  gather(outcome_var, value, -`Sleep disturbance`, -study) %>% 
  # Add variable group names
  left_join(variable_groups, by = "outcome_var")

  ggplot(plot_df) +
    aes(x = `Sleep disturbance`, y = value, color = as.factor(study)) +
    geom_point(alpha = .5) +
    geom_smooth(method = "lm", se = FALSE) +
    facet_wrap(~outcome_var, scales = "free") +
    scale_color_brewer(palette = "Set1") +
    labs(y = NULL, color = "Study") +
    theme_bw() +
    theme(legend.position = "bottom",
      panel.grid.major = element_blank(), 
      panel.grid.minor = element_blank(),
      panel.background = element_blank())
```

# References

